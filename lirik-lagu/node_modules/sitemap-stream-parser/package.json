{
  "_from": "sitemap-stream-parser",
  "_id": "sitemap-stream-parser@1.7.0",
  "_inBundle": false,
  "_integrity": "sha512-aGNRTohb0G9uhrS04C6NlTBRdCK7XzWpWEXKN5cUjiYkhbea+g6FWm3Js24Kw8EM+ryeZLM5fCPnPEEufwW4Hw==",
  "_location": "/sitemap-stream-parser",
  "_phantomChildren": {},
  "_requested": {
    "escapedName": "sitemap-stream-parser",
    "fetchSpec": "latest",
    "name": "sitemap-stream-parser",
    "raw": "sitemap-stream-parser",
    "rawSpec": "",
    "registry": true,
    "saveSpec": null,
    "type": "tag"
  },
  "_requiredBy": [
    "#USER",
    "/"
  ],
  "_resolved": "https://registry.npmjs.org/sitemap-stream-parser/-/sitemap-stream-parser-1.7.0.tgz",
  "_shasum": "37ed13698fcda1dfadba7b1e8120f0ac1ab16a2e",
  "_shrinkwrap": null,
  "_spec": "sitemap-stream-parser",
  "_where": "C:\\sitemap-parse\\liriklaguindonesia.net",
  "author": {
    "email": "erwin@404check.io",
    "name": "Erwin van der Koogh"
  },
  "bin": {
    "sitemap-stream-parser": "cli.js"
  },
  "bugs": {
    "url": "https://github.com/evanderkoogh/node-sitemap-stream-parser/issues"
  },
  "bundleDependencies": false,
  "dependencies": {
    "async": "^2.6.1",
    "commander": "^2.15.1",
    "request": "^2.87.0",
    "sax": "^1.2.4"
  },
  "deprecated": false,
  "description": "Get a list of URLs from one or more sitemaps",
  "devDependencies": {
    "coffeescript": "^2.3.2"
  },
  "homepage": "https://github.com/evanderkoogh/node-sitemap-stream-parser#readme",
  "keywords": [
    "multiple",
    "nested",
    "parser",
    "sitemap",
    "stream"
  ],
  "license": "Apache-2.0",
  "main": "index.js",
  "name": "sitemap-stream-parser",
  "optionalDependencies": {},
  "readme": "# node-sitemap-stream-parser\nA streaming parser for sitemap files. It is able to deal with GBs of deeply nested sitemaps with hundreds of URLs in them. Maximum memory usage is just over 100Mb at any time.\n\n## Usage\n\nThe main method to extract URLs for a site is with the `parseSitemaps(urls, url_cb, done)` method. You can call it with both a single URL or an Array of URLs. The `url_cb` is called for every URL that is found. The `done` callback is passed an error and/or a list of all the sitemaps that were checked.\n\n## Examples:\n\n``` javascript\nvar sitemaps = require('sitemap-stream-parser');\n\nsitemaps.parseSitemaps('http://example.com/sitemap.xml', console.log, function(err, sitemaps) {\n    console.log('All done!');\n});\n```\n\nor \n\n``` javascript\nvar sitemaps = require('sitemap-stream-parser');\n\nvar urls = ['http://example.com/sitemap-posts.xml', 'http://example.com/sitemap-pages.xml'];\n\nall_urls = [];\nsitemaps.parseSitemaps(urls, function(url) { all_urls.push(url); }, function(err, sitemaps) {\n    console.log(all_urls);\n    console.log('All done!');\n});\n```\n\nSometimes sites advertise their sitemaps in their `robots.txt` file. To parse this file to see if that is the case use the method `sitemapsInRobots(url, cb)`. You can easily combine those 2 methods.\n\n``` javascript\nvar sitemaps = require('sitemap-stream-parser');\n\nsitemaps.sitemapsInRobots('http://example.com/robots.txt', function(err, urls) {\n    if(err || !urls || urls.length == 0)\n        return;\n    sitemaps.parseSitemaps(urls, console.log, function(err, sitemaps) {\n        console.log(sitemaps);\n    });\n});\n",
  "readmeFilename": "README.md",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/evanderkoogh/node-sitemap-stream-parser.git"
  },
  "scripts": {
    "prepublish": "coffee -c index.coffee"
  },
  "version": "1.7.0"
}
